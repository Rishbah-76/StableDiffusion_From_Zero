{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn  import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self attention blockk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class selfAttention(nn.Module):\n",
    "    def __init__(self,n_heads:int,d_embed:int, in_proj_bias=True ,out_proj_bias=True):\n",
    "        super.__init__()\n",
    "        self.in_proj=nn.Linear(d_embed,3*d_embed,bias=in_proj_bias)\n",
    "        self.out_proj=nn.Linear(d_embed,d_embed,bias=out_proj_bias)\n",
    "        self.heads=n_heads\n",
    "        self.d_head=d_embed//n_heads\n",
    "\n",
    "    def forward(self, x, casual_mask=False):\n",
    "        # x: (Batch_size,seq_length (len_of_embed),Dim)\n",
    "        input_shape=x.shape\n",
    "        batch_size,seq_len,d_embed=input_shape\n",
    "\n",
    "        # required for converting the input into desired input for K,q,v\n",
    "        intermin_shape=(batch_size,seq_len,self.heads,self.d_head)\n",
    "\n",
    "        # (Batch_size,seq_length (len_of_embed),Dim)--> (Batch_size,seq_length (len_of_embed),Dim*3 )--> 3 tensors of shape  (Batch_size,seq_length (len_of_embed),Dim)\n",
    "        k,q,v=self.in_proj(x).chunk(3,dim=-1) # to the last dim which is itself in this case is \"DIM\"\n",
    "\n",
    "        # splitting the number of k,q,v in n_heads\n",
    "\n",
    "\n",
    "# transpose(): Swaps two dimensions of a tensor, effectively changing the arrangement of the data.\n",
    "# view(): Reshapes the tensor without changing its data. The number of elements must remain the same.\n",
    "\n",
    "#Creating Q, K, V:\n",
    "# For each word, we create Q, K, and V (in practice, these are learned linear transformations of the embeddings).\n",
    "\n",
    "        q=q.view(intermin_shape).transpose(1,2)  #(Batch_size,seq_length,Dim )--> (Batch_size,seq_length, Heads, DIM/Heads )\n",
    "        k=k.view(intermin_shape).transpose(1,2)  #(Batch_size,seq_length,Dim )--> (Batch_size,seq_length, Heads, DIM/Heads )\n",
    "        v=v.view(intermin_shape).transpose(1,2) #(Batch_size,seq_length,Dim )--> (Batch_size,seq_length, Heads, DIM/Heads )\n",
    "\n",
    "\n",
    "        # doing the weights \n",
    "        weights= q@k.transpose(-1,-2)\n",
    "\n",
    "        if casual_mask: # so that we can disaacoatiate simliars tokens nex to each other, makes a -infity passed to softmax--> gives 0\n",
    "            # mask where the upper traingles are 1 above the principal Diagonal\n",
    "            mask=torch.ones_like(weights,dtype=torch.bool).triu(1)\n",
    "            weights.masked_fill(mask,-torch.inf)\n",
    "\n",
    "        weights/=math.sqrt(self.d_head)\n",
    "        weights=F.softmax(weights,dim=-1)\n",
    "\n",
    "        output=weights@v # (Batch_size,Heads,Seq_len,Seq_len ) @ (Batch_size,Heads,Seq_len,DIM/Heads ) --> (Batch_size,Heads,Seq_len,DIM/Heads )\n",
    "        \n",
    "        output.transpose(1,2) #m (Batch_size , Seq_len, Heads, DIM/Heads )\n",
    "\n",
    "        output=output.reshape(input_shape)\n",
    "\n",
    "        output=self.out_proj(output)\n",
    "\n",
    "        # (Batch_size,seq_length,Dim )\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Clip Encoder code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
